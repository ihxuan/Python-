{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae22f378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ æ­£åœ¨ä¸‹è½½NLTKæ•°æ®...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/liuyixuan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/liuyixuan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/liuyixuan/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/liuyixuan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ä½¿ç”¨ä¸­æ–‡å­—ä½“: Hiragino Sans GB\n",
      "ğŸš€ ä½¿ç”¨å®Œæ•´æ¨¡å¼ï¼ˆåŒ…å«NLTKå’ŒTextBlobï¼‰\n",
      "============================================================\n",
      "ç¤¾äº¤åª’ä½“æ–‡æœ¬æƒ…æ„Ÿåˆ†æé¡¹ç›® - å®Œæ•´ç‰ˆ\n",
      "============================================================\n",
      "\n",
      "1. ğŸ“Š ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®...\n",
      "ç”Ÿæˆäº† 1000 æ¡ç¤¾äº¤åª’ä½“æ•°æ®\n",
      "\n",
      "2. ğŸ§¹ æ–‡æœ¬æ¸…æ´—...\n",
      "æ¸…æ´—åä¿ç•™ 1000 æ¡æœ‰æ•ˆæ•°æ®\n",
      "\n",
      "3. ğŸ˜Š æƒ…æ„Ÿåˆ†æ...\n",
      "\n",
      "4. ğŸ“ˆ ç»Ÿè®¡åˆ†æ...\n",
      "ğŸ“ˆ ç”Ÿæˆæƒ…æ„Ÿåˆ†å¸ƒå¯è§†åŒ–...\n",
      "ğŸ“ˆ ç”Ÿæˆæ—¶é—´åºåˆ—å¯è§†åŒ–...\n",
      "\n",
      "5. ğŸ“Š ç»Ÿè®¡è®¡ç®—åˆ†æ...\n",
      "æ–‡æœ¬é•¿åº¦ä¸æƒ…æ„Ÿå¾—åˆ†çš„ç›¸å…³ç³»æ•°: 0.0309\n",
      "æ´»è·ƒç”¨æˆ·å¹³å‡æƒ…æ„Ÿæ³¢åŠ¨æ€§: 0.6300\n",
      "\n",
      "6. ğŸ” ä¸»é¢˜å»ºæ¨¡...\n",
      "è¯†åˆ«å‡º 4 ä¸ªä¸»è¦ä¸»é¢˜ï¼š\n",
      "ä¸»é¢˜ 1: ä»Šå¤©å­¦åˆ°äº†æ–°çŸ¥è¯†æ„Ÿè§‰å¾ˆå……å®, çœŸç›¸å¾€å¾€åœ¨äº‰è®ºä¸­æµ®ç°, åˆ†äº«å¿«ä¹ä¼ é€’æ­£èƒ½é‡, å¯¹è¿™ä¸ªé—®é¢˜æœ‰ä¸åŒçœ‹æ³•, è¿™ä¸ªè®¨è®ºå¾ˆæœ‰æ„æ€\n",
      "ä¸»é¢˜ 2: å¥èº«è®©æˆ‘æ„Ÿè§‰å¾ˆæ£’, ä»Šå¤©å¤©æ°”çœŸå¥½å¿ƒæƒ…ä¸é”™, å·¥ä½œæœ‰ç‚¹ç´¯ä½†è¿˜ç®—é¡ºåˆ©, ç†æ€§è®¨è®ºå¾ˆé‡è¦, å¸Œæœ›æ»¡æ»¡æœªæ¥ä¸€å®šä¼šæ›´å¥½\n",
      "ä¸»é¢˜ 3: æ–°ä¹°çš„å’–å•¡è±†å¾ˆé¦™æ—©æ™¨å–å’–å•¡çœŸäº«å—, è¯»ä¹¦æ˜¯æœ€å¥½çš„æŠ•èµ„, éŸ³ä¹èƒ½å¤Ÿæ²»æ„ˆå¿ƒçµ, æ¯ä¸ªäººçš„è§‚ç‚¹éƒ½ä¸ä¸€æ ·, äººä»¬çš„ååº”å¾ˆæ¿€çƒˆ\n",
      "ä¸»é¢˜ 4: åˆšçœ‹å®Œä¸€éƒ¨å¾ˆæ£’çš„ç”µå½±æ¨èç»™å¤§å®¶, å’Œæœ‹å‹èšé¤å¾ˆå¼€å¿ƒçš„ä¸€å¤©, å‘¨æœ«è®¡åˆ’å»å…¬å›­æ•£æ­¥æ”¾æ¾ä¸€ä¸‹, è¿™ä¸ªè¯é¢˜å¾ˆå¤æ‚éœ€è¦æ·±å…¥æ€è€ƒ, è¿™æ˜¯å†å²æ€§çš„æ—¶åˆ»æˆ‘ä»¬è§è¯äº†å¥‡è¿¹\n",
      "\n",
      "7. ğŸ“ è¯é¢‘åˆ†æ...\n",
      "é«˜é¢‘è¯æ±‡ï¼ˆTop 10ï¼‰ï¼š\n",
      " 1. åˆšçœ‹å®Œä¸€éƒ¨å¾ˆæ£’çš„ç”µå½±æ¨èç»™å¤§å®¶ -  77 æ¬¡\n",
      " 2. å¥èº«è®©æˆ‘æ„Ÿè§‰å¾ˆæ£’ -  74 æ¬¡\n",
      " 3. ä»Šå¤©å¤©æ°”çœŸå¥½å¿ƒæƒ…ä¸é”™ -  63 æ¬¡\n",
      " 4. ä»Šå¤©å­¦åˆ°äº†æ–°çŸ¥è¯†æ„Ÿè§‰å¾ˆå……å® -  60 æ¬¡\n",
      " 5. æ–°ä¹°çš„å’–å•¡è±†å¾ˆé¦™æ—©æ™¨å–å’–å•¡çœŸäº«å— -  60 æ¬¡\n",
      " 6. è¯»ä¹¦æ˜¯æœ€å¥½çš„æŠ•èµ„ -  60 æ¬¡\n",
      " 7. å·¥ä½œæœ‰ç‚¹ç´¯ä½†è¿˜ç®—é¡ºåˆ© -  59 æ¬¡\n",
      " 8. å’Œæœ‹å‹èšé¤å¾ˆå¼€å¿ƒçš„ä¸€å¤© -  57 æ¬¡\n",
      " 9. å‘¨æœ«è®¡åˆ’å»å…¬å›­æ•£æ­¥æ”¾æ¾ä¸€ä¸‹ -  52 æ¬¡\n",
      "10. éŸ³ä¹èƒ½å¤Ÿæ²»æ„ˆå¿ƒçµ -  44 æ¬¡\n",
      "\n",
      "8. â˜ï¸ è¯äº‘å’Œç½‘ç»œå›¾...\n",
      "â˜ï¸ ç”Ÿæˆè¯äº‘...\n",
      "âœ… è¯äº‘ä½¿ç”¨å­—ä½“: /System/Library/Fonts/Hiragino Sans GB.ttc\n",
      "ğŸŒ ç”Ÿæˆå…±ç°ç½‘ç»œå›¾...\n",
      "\n",
      "9. ğŸ¤– äº¤äº’å¼æƒ…æ„Ÿåˆ†æå™¨æ¼”ç¤º...\n",
      "ğŸ­ æƒ…æ„Ÿåˆ†æå™¨å·²å¯åŠ¨ï¼(å®Œæ•´ç‰ˆ)\n",
      "æˆ‘å¯ä»¥å¸®åŠ©æ‚¨åˆ†ææ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ã€‚\n",
      "\n",
      "æƒ…æ„Ÿåˆ†æç»“æœï¼š\n",
      "æ–‡æœ¬: ä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œå¿ƒæƒ…å¾ˆæ£’ï¼\n",
      "æƒ…æ„Ÿ: æ­£é¢ ğŸ˜Š (å¾—åˆ†: 1.000)\n",
      "\n",
      "æ–‡æœ¬: è¿™ä¸ªæ¶ˆæ¯è®©æˆ‘å¾ˆå¤±æœ›\n",
      "æƒ…æ„Ÿ: è´Ÿé¢ ğŸ˜” (å¾—åˆ†: -1.000)\n",
      "\n",
      "æ–‡æœ¬: å·¥ä½œè¿˜ç®—æ­£å¸¸ï¼Œæ²¡ä»€ä¹ˆç‰¹åˆ«çš„\n",
      "æƒ…æ„Ÿ: ä¸­æ€§ ğŸ˜ (å¾—åˆ†: 0.000)\n",
      "\n",
      "æ–‡æœ¬: å¤ªæ¿€åŠ¨äº†ï¼ç»ˆäºæˆåŠŸäº†ï¼\n",
      "æƒ…æ„Ÿ: æ­£é¢ ğŸ˜Š (å¾—åˆ†: 1.000)\n",
      "\n",
      "æ–‡æœ¬: æœ‰ç‚¹æ‹…å¿ƒæ˜å¤©çš„è€ƒè¯•\n",
      "æƒ…æ„Ÿ: è´Ÿé¢ ğŸ˜” (å¾—åˆ†: -1.000)\n",
      "\n",
      "åˆ†æç»Ÿè®¡ï¼š\n",
      "æ€»åˆ†ææ¬¡æ•°: 5\n",
      "æƒ…æ„Ÿåˆ†å¸ƒ: {'æ­£é¢': 2, 'ä¸­æ€§': 1, 'è´Ÿé¢': 2}\n",
      "å¹³å‡æƒ…æ„Ÿå¾—åˆ†: 0.000\n",
      "\n",
      "ä¸ªæ€§åŒ–å»ºè®®ï¼š\n",
      "- âš–ï¸ æ‚¨çš„æƒ…æ„Ÿåˆ†ææ¯”è¾ƒå‡è¡¡ï¼Œè¿™å¾ˆå¥½ï¼\n",
      "- ğŸ“Š å¯ä»¥ç»§ç»­æ¢ç´¢ä¸åŒä¸»é¢˜æ–‡æœ¬çš„æƒ…æ„Ÿç‰¹å¾\n",
      "\n",
      "10. ğŸ“‹ æ•°æ®å¯é æ€§ä¸å±€é™æ€§åˆ†æ...\n",
      "==================================================\n",
      "æ•°æ®å¯é æ€§åˆ†æï¼š\n",
      "âœ… ä¼˜åŠ¿ï¼š\n",
      "  - æ•°æ®é‡å……è¶³ï¼Œæä¾›äº†è¾ƒå¥½çš„ç»Ÿè®¡åŸºç¡€\n",
      "  - æ—¶é—´è·¨åº¦è¾ƒé•¿ï¼Œèƒ½å¤Ÿè§‚å¯Ÿåˆ°è¶‹åŠ¿å˜åŒ–\n",
      "  - åŒ…å«å¤šå¹³å°æ•°æ®ï¼Œå¢åŠ äº†ä»£è¡¨æ€§\n",
      "\n",
      "âš ï¸ å±€é™æ€§ï¼š\n",
      "  - æ¨¡æ‹Ÿæ•°æ®åŸºäºé¢„è®¾æ¨¡æ¿ï¼Œç¼ºä¹çœŸå®å¤æ‚æ€§\n",
      "  - æƒ…æ„Ÿåˆ†æä¸»è¦é’ˆå¯¹è‹±æ–‡ä¼˜åŒ–ï¼Œä¸­æ–‡è¯†åˆ«å­˜åœ¨åå·®\n",
      "  - æ— æ³•è¯†åˆ«è®½åˆºã€åè¯­ç­‰å¤æ‚è¯­è¨€ç°è±¡\n",
      "  - ç›¸å…³æ€§ä¸ç­‰äºå› æœæ€§ï¼Œç¼ºä¹å› æœæ¨æ–­\n",
      "\n",
      "ğŸ‰ é¡¹ç›®å®Œæˆï¼\n",
      "==================================================\n",
      "ä½œä¸šè¦æ±‚å®ŒæˆçŠ¶å†µï¼š\n",
      "âœ… è®ºç‚¹æ„å»ºï¼ˆ20åˆ†ï¼‰ï¼šç¤¾äº¤åª’ä½“æƒ…æ„Ÿä¸ç¤¾ä¼šäº‹ä»¶å…³è”æ€§ç ”ç©¶\n",
      "âœ… æäº¤æ ¼å¼è§„èŒƒï¼ˆ20åˆ†ï¼‰ï¼šå®Œæ•´çš„å¯æ‰§è¡Œä»£ç \n",
      "âœ… ç»Ÿè®¡åˆ†æï¼ˆ20åˆ†ï¼‰ï¼š2ä¸ªå¯è§†åŒ– + 2é¡¹ç»Ÿè®¡è®¡ç®— + å¯é æ€§åˆ†æ\n",
      "âœ… æ–‡æœ¬åˆ†æï¼ˆ20åˆ†ï¼‰ï¼šæ¸…æ´—å‡½æ•° + ä¸»é¢˜å»ºæ¨¡ + è¯é¢‘åˆ†æ + è¯äº‘ + ç½‘ç»œå›¾\n",
      "âœ… äº¤äº’å¼Pythonç±»ï¼ˆ20åˆ†ï¼‰ï¼šSentimentAnalyzerç±» + å®Œæ•´æ¼”ç¤º\n",
      "==================================================\n",
      "è¿è¡Œæ¨¡å¼: å®Œæ•´ç‰ˆ (åŒ…å«NLTK)\n",
      "ç”Ÿæˆçš„æ–‡ä»¶ä¿å­˜åœ¨ output/ æ–‡ä»¶å¤¹ä¸­\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "ç¤¾äº¤åª’ä½“æ–‡æœ¬ä¸­çš„æƒ…æ„Ÿå˜åŒ–ä¸ç¤¾ä¼šäº‹ä»¶å…³è”æ€§ç ”ç©¶\n",
    "æ–‡æœ¬åˆ†æç»¼åˆé¡¹ç›® - ç»Ÿä¸€ç‰ˆæœ¬\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# 1. ä¾èµ–æ£€æŸ¥ä¸ç¯å¢ƒè®¾ç½®\n",
    "# ============================================================================\n",
    "\n",
    "def check_dependencies():\n",
    "    \"\"\"æ£€æŸ¥å¹¶å®‰è£…å¿…è¦çš„ä¾èµ–\"\"\"\n",
    "    required_packages = {\n",
    "        'pandas': 'pandas',\n",
    "        'numpy': 'numpy', \n",
    "        'matplotlib': 'matplotlib',\n",
    "        'seaborn': 'seaborn',\n",
    "        'wordcloud': 'wordcloud',\n",
    "        'sklearn': 'scikit-learn',\n",
    "        'networkx': 'networkx'\n",
    "    }\n",
    "    \n",
    "    optional_packages = {\n",
    "        'nltk': 'nltk',\n",
    "        'textblob': 'textblob'\n",
    "    }\n",
    "    \n",
    "    missing_packages = []\n",
    "    missing_optional = []\n",
    "    \n",
    "    # æ£€æŸ¥å¿…éœ€åŒ…\n",
    "    for package, pip_name in required_packages.items():\n",
    "        try:\n",
    "            __import__(package)\n",
    "        except ImportError:\n",
    "            missing_packages.append(pip_name)\n",
    "    \n",
    "    # æ£€æŸ¥å¯é€‰åŒ…\n",
    "    for package, pip_name in optional_packages.items():\n",
    "        try:\n",
    "            __import__(package)\n",
    "        except ImportError:\n",
    "            missing_optional.append(pip_name)\n",
    "    \n",
    "    if missing_packages:\n",
    "        print(\"âŒ å‘ç°ç¼ºå¤±çš„å¿…éœ€ä¾èµ–åŒ…:\")\n",
    "        for pkg in missing_packages:\n",
    "            print(f\"  - {pkg}\")\n",
    "        print(f\"\\nè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…:\")\n",
    "        print(f\"pip install {' '.join(missing_packages)}\")\n",
    "        return False, missing_optional\n",
    "    \n",
    "    if missing_optional:\n",
    "        print(\"âš ï¸ å‘ç°ç¼ºå¤±çš„å¯é€‰ä¾èµ–åŒ…ï¼ˆå°†ä½¿ç”¨è½»é‡ç‰ˆåŠŸèƒ½ï¼‰:\")\n",
    "        for pkg in missing_optional:\n",
    "            print(f\"  - {pkg}\")\n",
    "        print(\"å¯é€‰æ‹©å®‰è£…ä»¥è·å¾—å®Œæ•´åŠŸèƒ½ï¼Œæˆ–ç»§ç»­ä½¿ç”¨è½»é‡ç‰ˆ\")\n",
    "    \n",
    "    return True, missing_optional\n",
    "\n",
    "# æ£€æŸ¥ä¾èµ–\n",
    "deps_ok, missing_nlp = check_dependencies()\n",
    "if not deps_ok:\n",
    "    sys.exit(1)\n",
    "\n",
    "# æ ¹æ®ä¾èµ–æƒ…å†µå†³å®šä½¿ç”¨æ¨¡å¼\n",
    "USE_LITE_MODE = len(missing_nlp) > 0\n",
    "\n",
    "# åˆå§‹æ¨¡å¼æç¤ºä¼šåœ¨NLTKæ£€æŸ¥åæ›´æ–°\n",
    "\n",
    "# å¯¼å…¥åŸºç¡€åº“\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# è®¾ç½®matplotlibåç«¯\n",
    "try:\n",
    "    matplotlib.use('TkAgg')\n",
    "except:\n",
    "    try:\n",
    "        matplotlib.use('Qt5Agg')\n",
    "    except:\n",
    "        matplotlib.use('Agg')\n",
    "\n",
    "# NLPç›¸å…³åº“ï¼ˆå¯é€‰ï¼‰\n",
    "if not USE_LITE_MODE:\n",
    "    try:\n",
    "        import nltk\n",
    "        from textblob import TextBlob\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        from sklearn.decomposition import LatentDirichletAllocation\n",
    "        import networkx as nx\n",
    "        import nltk\n",
    "        import ssl\n",
    "\n",
    "        try:\n",
    "            _create_unverified_https_context = ssl._create_unverified_context\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        else:\n",
    "            ssl._create_default_https_context = _create_unverified_https_context\n",
    "        \n",
    "        # å°è¯•ä¸‹è½½NLTKæ•°æ®\n",
    "        nltk_data_available = True\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "            nltk.data.find('corpora/stopwords')\n",
    "            nltk.data.find('vader_lexicon')\n",
    "            nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "        except LookupError:\n",
    "            print(\"ğŸ”„ æ­£åœ¨ä¸‹è½½NLTKæ•°æ®...\")\n",
    "            # å°è¯•ä¸‹è½½ï¼Œä½†ä¸ä½¿ç”¨quietæ¨¡å¼ä»¥ä¾¿æ•è·SSLé”™è¯¯\n",
    "            download_success = True\n",
    "            for data_name in ['punkt', 'stopwords', 'vader_lexicon', 'averaged_perceptron_tagger']:\n",
    "                try:\n",
    "                    result = nltk.download(data_name)\n",
    "                    if not result:\n",
    "                        download_success = False\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ ä¸‹è½½ {data_name} å¤±è´¥: {e}\")\n",
    "                    download_success = False\n",
    "                    break\n",
    "            \n",
    "            if not download_success:\n",
    "                print(\"âš ï¸ NLTKæ•°æ®ä¸‹è½½å¤±è´¥ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°è½»é‡æ¨¡å¼\")\n",
    "                print(\"ğŸ’¡ æç¤ºï¼šè¿è¡Œ 'python3 fix_ssl.py' å¯ä»¥å°è¯•ä¿®å¤SSLé—®é¢˜\")\n",
    "                nltk_data_available = False\n",
    "        \n",
    "        if not nltk_data_available:\n",
    "            USE_LITE_MODE = True\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ NLPåº“å¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨è½»é‡æ¨¡å¼: {e}\")\n",
    "        USE_LITE_MODE = True\n",
    "# ç¡®ä¿å¯ä»¥å¯¼å…¥sklearn\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "    import networkx as nx\n",
    "except ImportError:\n",
    "    print(\"âŒ ç¼ºå°‘scikit-learnæˆ–networkxï¼Œè¯·å®‰è£…\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“\n",
    "def setup_chinese_fonts():\n",
    "    \"\"\"è®¾ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º\"\"\"\n",
    "    try:\n",
    "        # è·å–ç³»ç»Ÿæ‰€æœ‰å¯ç”¨å­—ä½“\n",
    "        available_fonts = [f.name for f in matplotlib.font_manager.fontManager.ttflist]\n",
    "        \n",
    "        # macOSä¸­æ–‡å­—ä½“\n",
    "        macos_fonts = [\n",
    "            'PingFang SC', 'Hiragino Sans GB', 'STHeiti', 'STSong', \n",
    "            'Songti SC', 'Heiti SC', 'Arial Unicode MS'\n",
    "        ]\n",
    "        \n",
    "        # Windowsä¸­æ–‡å­—ä½“\n",
    "        windows_fonts = [\n",
    "            'SimHei', 'Microsoft YaHei', 'SimSun', 'KaiTi', \n",
    "            'FangSong', 'Microsoft JhengHei'\n",
    "        ]\n",
    "        \n",
    "        # Linuxä¸­æ–‡å­—ä½“\n",
    "        linux_fonts = [\n",
    "            'WenQuanYi Micro Hei', 'WenQuanYi Zen Hei', 'Noto Sans CJK SC',\n",
    "            'Source Han Sans CN', 'Droid Sans Fallback'\n",
    "        ]\n",
    "        \n",
    "        # åˆå¹¶æ‰€æœ‰å­—ä½“åˆ—è¡¨\n",
    "        all_chinese_fonts = macos_fonts + windows_fonts + linux_fonts\n",
    "        \n",
    "        font_found = False\n",
    "        selected_font = None\n",
    "        for font in all_chinese_fonts:\n",
    "            if font in available_fonts:\n",
    "                plt.rcParams['font.sans-serif'] = [font]\n",
    "                selected_font = font\n",
    "                print(f\"âœ… ä½¿ç”¨ä¸­æ–‡å­—ä½“: {font}\")\n",
    "                font_found = True\n",
    "                break\n",
    "        \n",
    "        if not font_found:\n",
    "            print(\"âš ï¸ æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œå°è¯•ä½¿ç”¨ç³»ç»Ÿé»˜è®¤å­—ä½“\")\n",
    "            # å°è¯•ä½¿ç”¨åŒ…å«CJKçš„å­—ä½“\n",
    "            for font_name in available_fonts:\n",
    "                if any(keyword in font_name.lower() for keyword in ['cjk', 'han', 'chinese', 'zh', 'song', 'hei']):\n",
    "                    plt.rcParams['font.sans-serif'] = [font_name]\n",
    "                    selected_font = font_name\n",
    "                    print(f\"âœ… æ‰¾åˆ°å¯èƒ½æ”¯æŒä¸­æ–‡çš„å­—ä½“: {font_name}\")\n",
    "                    font_found = True\n",
    "                    break\n",
    "        \n",
    "        if not font_found:\n",
    "            print(\"âŒ æ— æ³•æ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œå›¾è¡¨ä¸­æ–‡å°†æ˜¾ç¤ºä¸ºæ–¹å—\")\n",
    "            print(\"ğŸ’¡ å»ºè®®ï¼šå®‰è£…ä¸­æ–‡å­—ä½“æˆ–ä½¿ç”¨è‹±æ–‡æ ‡ç­¾\")\n",
    "            selected_font = None\n",
    "            \n",
    "        # è®¾ç½®è´Ÿå·æ­£å¸¸æ˜¾ç¤º\n",
    "        plt.rcParams['axes.unicode_minus'] = False\n",
    "        \n",
    "        # è®¾ç½®å­—ä½“å¤§å°\n",
    "        plt.rcParams['font.size'] = 12\n",
    "        plt.rcParams['axes.titlesize'] = 14\n",
    "        plt.rcParams['axes.labelsize'] = 12\n",
    "        \n",
    "        # è¿”å›å­—ä½“å±æ€§å¯¹è±¡ï¼Œä¾›æ ‡é¢˜å’Œæ ‡ç­¾ä½¿ç”¨\n",
    "        if selected_font:\n",
    "            return matplotlib.font_manager.FontProperties(family=selected_font)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ å­—ä½“è®¾ç½®å¤±è´¥: {e}\")\n",
    "        return None\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“å¹¶è·å–å­—ä½“å±æ€§\n",
    "chinese_font_prop = setup_chinese_fonts()\n",
    "\n",
    "# è®¾ç½®å›¾è¡¨æ ·å¼\n",
    "plt.style.use('default')\n",
    "try:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_palette(\"husl\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# æ˜¾ç¤ºæœ€ç»ˆè¿è¡Œæ¨¡å¼\n",
    "if USE_LITE_MODE:\n",
    "    print(\"ğŸ”„ ä½¿ç”¨è½»é‡æ¨¡å¼ï¼ˆåŸºäºè¯å…¸çš„æƒ…æ„Ÿåˆ†æï¼‰\")\n",
    "else:\n",
    "    print(\"ğŸš€ ä½¿ç”¨å®Œæ•´æ¨¡å¼ï¼ˆåŒ…å«NLTKå’ŒTextBlobï¼‰\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"ç¤¾äº¤åª’ä½“æ–‡æœ¬æƒ…æ„Ÿåˆ†æé¡¹ç›® {'- è½»é‡ç‰ˆ' if USE_LITE_MODE else '- å®Œæ•´ç‰ˆ'}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================================\n",
    "# 2. æƒ…æ„Ÿåˆ†ææ¨¡å—\n",
    "# ============================================================================\n",
    "\n",
    "# ä¸­æ–‡æƒ…æ„Ÿè¯å…¸ï¼ˆç”¨äºè½»é‡æ¨¡å¼ï¼‰\n",
    "POSITIVE_WORDS = {\n",
    "    'å¥½', 'æ£’', 'ä¼˜ç§€', 'æˆåŠŸ', 'å¼€å¿ƒ', 'é«˜å…´', 'å¿«ä¹', 'å…´å¥‹', 'æ¿€åŠ¨', 'æ»¡æ„',\n",
    "    'å–œæ¬¢', 'çˆ±', 'ç¾å¥½', 'å®Œç¾', 'èµ', 'ç‰›', 'å‰å®³', 'ä¸é”™', 'å¤ªå¥½äº†', 'ç»™åŠ›',\n",
    "    'ä¼˜', 'ä½³', 'å¦™', 'ç²¾å½©', 'æƒŠå–œ', 'æ„ŸåŠ¨', 'æ¸©æš–', 'èˆ’æœ', 'æ»¡è¶³', 'å¹¸ç¦'\n",
    "}\n",
    "\n",
    "NEGATIVE_WORDS = {\n",
    "    'å', 'å·®', 'ç³Ÿç³•', 'å¤±è´¥', 'éš¾è¿‡', 'ä¼¤å¿ƒ', 'ç—›è‹¦', 'å¤±æœ›', 'æ²®ä¸§', 'éƒé—·',\n",
    "    'è®¨åŒ', 'æ¨', 'çƒ¦', 'ç´¯', 'ç–²æƒ«', 'ç„¦è™‘', 'æ‹…å¿ƒ', 'å®³æ€•', 'ææƒ§', 'æ„¤æ€’',\n",
    "    'ç”Ÿæ°”', 'ä¸æ»¡', 'æŠ±æ€¨', 'æ‰¹è¯„', 'é—®é¢˜', 'é”™è¯¯', 'éº»çƒ¦', 'å›°éš¾', 'å‹åŠ›', 'ç´§å¼ '\n",
    "}\n",
    "\n",
    "def simple_sentiment_analysis(text):\n",
    "    \"\"\"åŸºäºè¯å…¸çš„ç®€åŒ–æƒ…æ„Ÿåˆ†æ\"\"\"\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    \n",
    "    # æå–ä¸­æ–‡è¯æ±‡ï¼ŒåŒ…æ‹¬å•å­—å’Œè¯ç»„\n",
    "    words = []\n",
    "    # 2-3å­—è¯ç»„\n",
    "    for i in range(len(text) - 1):\n",
    "        word2 = text[i:i+2]\n",
    "        if re.match(r'^[\\u4e00-\\u9fa5]{2}$', word2):\n",
    "            words.append(word2)\n",
    "        if i < len(text) - 2:\n",
    "            word3 = text[i:i+3]\n",
    "            if re.match(r'^[\\u4e00-\\u9fa5]{3}$', word3):\n",
    "                words.append(word3)\n",
    "    \n",
    "    # å•å­—\n",
    "    single_chars = re.findall(r'[\\u4e00-\\u9fa5]', text)\n",
    "    words.extend(single_chars)\n",
    "    \n",
    "    # æ£€æŸ¥åŒ…å«å…³ç³»\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    \n",
    "    # æ£€æŸ¥å®Œæ•´è¯æ±‡åŒ¹é…\n",
    "    for word in POSITIVE_WORDS:\n",
    "        if word in text:\n",
    "            positive_count += text.count(word)\n",
    "    \n",
    "    for word in NEGATIVE_WORDS:\n",
    "        if word in text:\n",
    "            negative_count += text.count(word)\n",
    "    \n",
    "    # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°å®Œæ•´è¯æ±‡ï¼Œæ£€æŸ¥å•å­—\n",
    "    if positive_count == 0 and negative_count == 0:\n",
    "        for char in single_chars:\n",
    "            if char in POSITIVE_WORDS:\n",
    "                positive_count += 1\n",
    "            elif char in NEGATIVE_WORDS:\n",
    "                negative_count += 1\n",
    "    \n",
    "    if positive_count == 0 and negative_count == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # è®¡ç®—æƒ…æ„Ÿå¾—åˆ†\n",
    "    total_sentiment_words = positive_count + negative_count\n",
    "    if total_sentiment_words == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    sentiment_score = (positive_count - negative_count) / total_sentiment_words\n",
    "    return max(-1, min(1, sentiment_score))\n",
    "\n",
    "def textblob_sentiment_analysis(text):\n",
    "    \"\"\"åŸºäºTextBlobçš„æƒ…æ„Ÿåˆ†æ\"\"\"\n",
    "    try:\n",
    "        blob = TextBlob(text)\n",
    "        return blob.sentiment.polarity\n",
    "    except:\n",
    "        return simple_sentiment_analysis(text)\n",
    "\n",
    "def get_sentiment_score(text):\n",
    "    \"\"\"ç»Ÿä¸€çš„æƒ…æ„Ÿåˆ†ææ¥å£\"\"\"\n",
    "    # å¯¹äºä¸­æ–‡æ–‡æœ¬ï¼Œæ€»æ˜¯ä½¿ç”¨æˆ‘ä»¬çš„è¯å…¸åˆ†æï¼Œå› ä¸ºæ•ˆæœæ›´å¥½\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    \n",
    "    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦\n",
    "    has_chinese = bool(re.search(r'[\\u4e00-\\u9fa5]', text))\n",
    "    \n",
    "    if has_chinese:\n",
    "        return simple_sentiment_analysis(text)\n",
    "    else:\n",
    "        # å¯¹äºè‹±æ–‡æ–‡æœ¬ï¼Œå¦‚æœæœ‰TextBlobåˆ™ä½¿ç”¨TextBlob\n",
    "        if not USE_LITE_MODE:\n",
    "            return textblob_sentiment_analysis(text)\n",
    "        else:\n",
    "            return simple_sentiment_analysis(text)\n",
    "\n",
    "# ============================================================================\n",
    "# 3. æ•°æ®ç”Ÿæˆå‡½æ•°\n",
    "# ============================================================================\n",
    "\n",
    "def generate_social_media_data(n_posts=1000):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆæ¨¡æ‹Ÿçš„ç¤¾äº¤åª’ä½“æ•°æ®ï¼ŒåŒ…å«ä¸åŒæ—¶æœŸå’Œä¸»é¢˜çš„å¸–å­\n",
    "    \n",
    "    Args:\n",
    "        n_posts (int): è¦ç”Ÿæˆçš„å¸–å­æ•°é‡\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: åŒ…å«æ–‡æœ¬ã€æ—¶é—´æˆ³ã€äº‹ä»¶ç±»å‹ç­‰å­—æ®µçš„æ•°æ®æ¡†\n",
    "    \"\"\"\n",
    "    \n",
    "    event_texts = {\n",
    "        'normal': [\n",
    "            \"ä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œå¿ƒæƒ…ä¸é”™\",\n",
    "            \"åˆšçœ‹å®Œä¸€éƒ¨å¾ˆæ£’çš„ç”µå½±ï¼Œæ¨èç»™å¤§å®¶\", \n",
    "            \"å’Œæœ‹å‹èšé¤ï¼Œå¾ˆå¼€å¿ƒçš„ä¸€å¤©\",\n",
    "            \"å·¥ä½œæœ‰ç‚¹ç´¯ï¼Œä½†è¿˜ç®—é¡ºåˆ©\",\n",
    "            \"å‘¨æœ«è®¡åˆ’å»å…¬å›­æ•£æ­¥ï¼Œæ”¾æ¾ä¸€ä¸‹\",\n",
    "            \"æ–°ä¹°çš„å’–å•¡è±†å¾ˆé¦™ï¼Œæ—©æ™¨å–å’–å•¡çœŸäº«å—\",\n",
    "            \"ä»Šå¤©å­¦åˆ°äº†æ–°çŸ¥è¯†ï¼Œæ„Ÿè§‰å¾ˆå……å®\",\n",
    "            \"è¯»ä¹¦æ˜¯æœ€å¥½çš„æŠ•èµ„\",\n",
    "            \"å¥èº«è®©æˆ‘æ„Ÿè§‰å¾ˆæ£’\",\n",
    "            \"éŸ³ä¹èƒ½å¤Ÿæ²»æ„ˆå¿ƒçµ\"\n",
    "        ],\n",
    "        'positive_event': [\n",
    "            \"å¤ªæ¿€åŠ¨äº†ï¼è¿™ä¸ªæ¶ˆæ¯çœŸæ˜¯å¤ªæ£’äº†ï¼\",\n",
    "            \"ç»ˆäºç­‰åˆ°è¿™ä¸€åˆ»ï¼Œæ„ŸåŠ¨å¾—è¦å“­äº†\",\n",
    "            \"è¿™æ˜¯å†å²æ€§çš„æ—¶åˆ»ï¼Œæˆ‘ä»¬è§è¯äº†å¥‡è¿¹\",\n",
    "            \"å…¨ä¸–ç•Œéƒ½åœ¨åº†ç¥ï¼Œæˆ‘ä¹Ÿå¾ˆå¼€å¿ƒ\",\n",
    "            \"å¸Œæœ›æ»¡æ»¡ï¼Œæœªæ¥ä¸€å®šä¼šæ›´å¥½\",\n",
    "            \"è¿™ä¸ªå¥½æ¶ˆæ¯è®©æˆ‘ä¸€æ•´å¤©éƒ½å¾ˆå…´å¥‹\",\n",
    "            \"åˆ†äº«å¿«ä¹ï¼Œä¼ é€’æ­£èƒ½é‡\",\n",
    "            \"æ¢¦æƒ³æˆçœŸçš„æ„Ÿè§‰çœŸå¥½\",\n",
    "            \"æˆåŠŸæ€»æ˜¯ç»™æœ‰å‡†å¤‡çš„äºº\",\n",
    "            \"ä»Šå¤©æ˜¯æœ€ç¾å¥½çš„ä¸€å¤©\"\n",
    "        ],\n",
    "        'negative_event': [\n",
    "            \"è¿™ä¸ªæ¶ˆæ¯è®©äººå¾ˆéš¾è¿‡\",\n",
    "            \"ä¸ºä»€ä¹ˆä¼šå‘ç”Ÿè¿™æ ·çš„äº‹æƒ…\",\n",
    "            \"å¿ƒæƒ…å¾ˆæ²‰é‡ï¼Œå¸Œæœ›ä¸€åˆ‡éƒ½ä¼šå¥½èµ·æ¥\",\n",
    "            \"è¿™çœŸçš„å¾ˆä»¤äººå¤±æœ›\",\n",
    "            \"éœ€è¦æ—¶é—´æ¥æ¶ˆåŒ–è¿™ä¸ªæ¶ˆæ¯\",\n",
    "            \"æ„Ÿåˆ°å¾ˆç„¦è™‘å’Œæ‹…å¿ƒ\",\n",
    "            \"å¸Œæœ›æƒ…å†µèƒ½å¤Ÿæ”¹å–„\",\n",
    "            \"ç”Ÿæ´»æœ‰æ—¶å€™çœŸçš„å¾ˆä¸å®¹æ˜“\",\n",
    "            \"é¢å¯¹å›°éš¾ï¼Œæˆ‘ä»¬è¦åšå¼º\",\n",
    "            \"é»‘æš—è¿‡åæ€»ä¼šæœ‰å…‰æ˜\"\n",
    "        ],\n",
    "        'controversy': [\n",
    "            \"å¯¹è¿™ä¸ªé—®é¢˜æœ‰ä¸åŒçœ‹æ³•\",\n",
    "            \"è¿™ä¸ªè¯é¢˜å¾ˆå¤æ‚ï¼Œéœ€è¦æ·±å…¥æ€è€ƒ\",\n",
    "            \"æ¯ä¸ªäººçš„è§‚ç‚¹éƒ½ä¸ä¸€æ ·\",\n",
    "            \"è¿™ç¡®å®æ˜¯ä¸ªäº‰è®®æ€§çš„è¯é¢˜\",\n",
    "            \"éœ€è¦æ›´å¤šä¿¡æ¯æ‰èƒ½åˆ¤æ–­\",\n",
    "            \"äººä»¬çš„ååº”å¾ˆæ¿€çƒˆ\",\n",
    "            \"è¿™ä¸ªè®¨è®ºå¾ˆæœ‰æ„æ€\",\n",
    "            \"è§‚ç‚¹çš„å¤šæ ·æ€§æ˜¯å¥½äº‹\",\n",
    "            \"ç†æ€§è®¨è®ºå¾ˆé‡è¦\",\n",
    "            \"çœŸç›¸å¾€å¾€åœ¨äº‰è®ºä¸­æµ®ç°\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    start_date = datetime.now() - timedelta(days=365)\n",
    "    data = []\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    for i in range(n_posts):\n",
    "        days_from_start = random.randint(0, 365)\n",
    "        \n",
    "        if days_from_start < 100:\n",
    "            event_type = np.random.choice(['normal', 'negative_event'], p=[0.7, 0.3])\n",
    "        elif days_from_start < 200:\n",
    "            event_type = np.random.choice(['normal', 'positive_event'], p=[0.6, 0.4])\n",
    "        elif days_from_start < 300:\n",
    "            event_type = np.random.choice(['normal', 'controversy'], p=[0.5, 0.5])\n",
    "        else:\n",
    "            event_type = np.random.choice(list(event_texts.keys()), p=[0.6, 0.15, 0.15, 0.1])\n",
    "        \n",
    "        text = random.choice(event_texts[event_type])\n",
    "        timestamp = start_date + timedelta(days=days_from_start, \n",
    "                                         hours=random.randint(0, 23),\n",
    "                                         minutes=random.randint(0, 59))\n",
    "        \n",
    "        data.append({\n",
    "            'text': text,\n",
    "            'timestamp': timestamp,\n",
    "            'event_type': event_type,\n",
    "            'user_id': f\"user_{random.randint(1, 200)}\",\n",
    "            'platform': random.choice(['Twitter', 'Facebook', 'Instagram', 'WeChat'])\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# ============================================================================\n",
    "# 4. æ–‡æœ¬æ¸…æ´—å‡½æ•°ï¼ˆæ»¡è¶³ä½œä¸šè¦æ±‚ï¼‰\n",
    "# ============================================================================\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    æ¸…æ´—æ–‡æœ¬æ•°æ®çš„å‡½æ•°\n",
    "    \n",
    "    è¯¥å‡½æ•°æ‰§è¡Œä»¥ä¸‹æ¸…æ´—æ­¥éª¤ï¼š\n",
    "    1. è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼\n",
    "    2. ç§»é™¤URLé“¾æ¥\n",
    "    3. ç§»é™¤ç”¨æˆ·åå’Œæ ‡ç­¾\n",
    "    4. ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼ˆä¿ç•™ä¸­æ–‡å­—ç¬¦ï¼‰\n",
    "    5. ç§»é™¤å¤šä½™ç©ºæ ¼\n",
    "    6. å¤„ç†ç©ºå€¼\n",
    "    \n",
    "    Args:\n",
    "        text (str): å¾…æ¸…æ´—çš„åŸå§‹æ–‡æœ¬\n",
    "        \n",
    "    Returns:\n",
    "        str: æ¸…æ´—åçš„æ–‡æœ¬\n",
    "        \n",
    "    Examples:\n",
    "        >>> clean_text(\"ä»Šå¤©å¤©æ°”çœŸå¥½ï¼ï¼ï¼@user #æ ‡ç­¾\")\n",
    "        'ä»Šå¤©å¤©æ°”çœŸå¥½'\n",
    "        \n",
    "        >>> clean_text(\"https://example.com è¿™æ˜¯ä¸€ä¸ªé“¾æ¥\")\n",
    "        'è¿™æ˜¯ä¸€ä¸ªé“¾æ¥'\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    text = re.sub(r'[^\\u4e00-\\u9fa5\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# ============================================================================\n",
    "# 5. ä¸»é¢˜å»ºæ¨¡å‡½æ•°\n",
    "# ============================================================================\n",
    "\n",
    "def perform_topic_modeling(texts, n_topics=4, max_features=100):\n",
    "    \"\"\"\n",
    "    æ‰§è¡ŒLDAä¸»é¢˜å»ºæ¨¡\n",
    "    \n",
    "    Args:\n",
    "        texts (list): æ¸…æ´—åçš„æ–‡æœ¬åˆ—è¡¨\n",
    "        n_topics (int): ä¸»é¢˜æ•°é‡\n",
    "        max_features (int): æœ€å¤§ç‰¹å¾æ•°é‡\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (LDAæ¨¡å‹, å‘é‡åŒ–å™¨, ä¸»é¢˜è¯æ±‡)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        texts = [text for text in texts if len(text) > 5]\n",
    "        \n",
    "        if len(texts) == 0:\n",
    "            print(\"âš ï¸ æ²¡æœ‰æœ‰æ•ˆæ–‡æœ¬ç”¨äºä¸»é¢˜å»ºæ¨¡\")\n",
    "            return None, None, []\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=(1, 2))\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        \n",
    "        lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42, max_iter=10)\n",
    "        lda_model.fit(tfidf_matrix)\n",
    "        \n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        topics = []\n",
    "        \n",
    "        for topic_idx, topic in enumerate(lda_model.components_):\n",
    "            top_words_idx = topic.argsort()[-10:][::-1]\n",
    "            top_words = [feature_names[i] for i in top_words_idx]\n",
    "            topics.append(top_words)\n",
    "        \n",
    "        return lda_model, vectorizer, topics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ä¸»é¢˜å»ºæ¨¡å¤±è´¥: {e}\")\n",
    "        return None, None, []\n",
    "\n",
    "# ============================================================================\n",
    "# 6. äº¤äº’å¼Pythonç±»ï¼ˆæ»¡è¶³ä½œä¸šè¦æ±‚ï¼‰\n",
    "# ============================================================================\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    äº¤äº’å¼æƒ…æ„Ÿåˆ†æå™¨ç±»\n",
    "    \n",
    "    è¯¥ç±»æä¾›äº¤äº’å¼çš„æ–‡æœ¬æƒ…æ„Ÿåˆ†æåŠŸèƒ½ï¼ŒåŒ…æ‹¬:\n",
    "    - å•æ–‡æœ¬æƒ…æ„Ÿåˆ†æ\n",
    "    - å†å²è®°å½•ç®¡ç†\n",
    "    - æƒ…æ„Ÿç»Ÿè®¡åˆ†æ\n",
    "    - ä¸ªæ€§åŒ–å»ºè®®ç”Ÿæˆ\n",
    "    \n",
    "    Attributes:\n",
    "        analysis_history (list): åˆ†æå†å²è®°å½•\n",
    "        total_analyses (int): æ€»åˆ†ææ¬¡æ•°\n",
    "        sentiment_counts (dict): å„æƒ…æ„Ÿç±»åˆ«è®¡æ•°\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"åˆå§‹åŒ–æƒ…æ„Ÿåˆ†æå™¨\"\"\"\n",
    "        self.analysis_history = []\n",
    "        self.total_analyses = 0\n",
    "        self.sentiment_counts = {'æ­£é¢': 0, 'ä¸­æ€§': 0, 'è´Ÿé¢': 0}\n",
    "        \n",
    "        mode = \"è½»é‡ç‰ˆ\" if USE_LITE_MODE else \"å®Œæ•´ç‰ˆ\"\n",
    "        print(f\"ğŸ­ æƒ…æ„Ÿåˆ†æå™¨å·²å¯åŠ¨ï¼({mode})\")\n",
    "        print(\"æˆ‘å¯ä»¥å¸®åŠ©æ‚¨åˆ†ææ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ã€‚\")\n",
    "    \n",
    "    def analyze_sentiment(self, text):\n",
    "        \"\"\"\n",
    "        åˆ†æå•ä¸ªæ–‡æœ¬çš„æƒ…æ„Ÿ\n",
    "        \n",
    "        Args:\n",
    "            text (str): å¾…åˆ†æçš„æ–‡æœ¬\n",
    "        \n",
    "        Returns:\n",
    "            dict: åŒ…å«æƒ…æ„Ÿåˆ†æç»“æœçš„å­—å…¸\n",
    "        \"\"\"\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return {'error': 'æ–‡æœ¬ä¸èƒ½ä¸ºç©º'}\n",
    "        \n",
    "        try:\n",
    "            polarity = get_sentiment_score(text)\n",
    "            \n",
    "            if polarity > 0.1:\n",
    "                sentiment_label = 'æ­£é¢'\n",
    "                emoji = 'ğŸ˜Š'\n",
    "            elif polarity < -0.1:\n",
    "                sentiment_label = 'è´Ÿé¢'\n",
    "                emoji = 'ğŸ˜”'\n",
    "            else:\n",
    "                sentiment_label = 'ä¸­æ€§'\n",
    "                emoji = 'ğŸ˜'\n",
    "            \n",
    "            result = {\n",
    "                'text': text,\n",
    "                'sentiment_score': polarity,\n",
    "                'sentiment_label': sentiment_label,\n",
    "                'emoji': emoji,\n",
    "                'confidence': abs(polarity),\n",
    "                'timestamp': datetime.now()\n",
    "            }\n",
    "            \n",
    "            self.analysis_history.append(result)\n",
    "            self.total_analyses += 1\n",
    "            self.sentiment_counts[sentiment_label] += 1\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'error': f'åˆ†æå¤±è´¥: {str(e)}'}\n",
    "    \n",
    "    def batch_analyze(self, texts):\n",
    "        \"\"\"æ‰¹é‡åˆ†ææ–‡æœ¬\"\"\"\n",
    "        results = []\n",
    "        for text in texts:\n",
    "            result = self.analyze_sentiment(text)\n",
    "            results.append(result)\n",
    "        return results\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"è·å–ç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
    "        if self.total_analyses == 0:\n",
    "            return {'total_analyses': 0, 'message': 'æš‚æ— åˆ†ææ•°æ®'}\n",
    "        \n",
    "        valid_scores = [r['sentiment_score'] for r in self.analysis_history if 'sentiment_score' in r]\n",
    "        avg_sentiment = np.mean(valid_scores) if valid_scores else 0\n",
    "        \n",
    "        return {\n",
    "            'total_analyses': self.total_analyses,\n",
    "            'sentiment_counts': self.sentiment_counts.copy(),\n",
    "            'avg_sentiment': avg_sentiment,\n",
    "            'history_length': len(self.analysis_history)\n",
    "        }\n",
    "    \n",
    "    def get_personalized_suggestions(self):\n",
    "        \"\"\"æ ¹æ®ç”¨æˆ·çš„åˆ†æå†å²æä¾›ä¸ªæ€§åŒ–å»ºè®®\"\"\"\n",
    "        if self.total_analyses == 0:\n",
    "            return ['è¯·å…ˆè¿›è¡Œä¸€äº›æ–‡æœ¬åˆ†æ']\n",
    "        \n",
    "        positive_ratio = self.sentiment_counts['æ­£é¢'] / self.total_analyses\n",
    "        negative_ratio = self.sentiment_counts['è´Ÿé¢'] / self.total_analyses\n",
    "        \n",
    "        suggestions = []\n",
    "        \n",
    "        if positive_ratio > 0.6:\n",
    "            suggestions.append(\"ğŸŒŸ æ‚¨çš„æ–‡æœ¬å¤§å¤šè¡¨è¾¾æ­£é¢æƒ…æ„Ÿï¼Œä¿æŒç§¯æçš„å¿ƒæ€ï¼\")\n",
    "            suggestions.append(\"ğŸ’­ å¯ä»¥å°è¯•åˆ†æä¸€äº›ä¸åŒç±»å‹çš„æ–‡æœ¬æ¥æ¢ç´¢æƒ…æ„Ÿè¡¨è¾¾çš„å¤šæ ·æ€§\")\n",
    "        \n",
    "        elif negative_ratio > 0.5:\n",
    "            suggestions.append(\"ğŸŒˆ æ³¨æ„åˆ°æ‚¨åˆ†æçš„æ–‡æœ¬åå‘è´Ÿé¢ï¼Œå¯ä»¥å¤šå…³æ³¨ç§¯æå†…å®¹\")\n",
    "            suggestions.append(\"âœ¨ å»ºè®®åˆ†æä¸€äº›æ­£é¢æ–°é—»æˆ–åŠ±å¿—æ–‡æœ¬æ¥å¹³è¡¡æƒ…æ„Ÿ\")\n",
    "        \n",
    "        else:\n",
    "            suggestions.append(\"âš–ï¸ æ‚¨çš„æƒ…æ„Ÿåˆ†ææ¯”è¾ƒå‡è¡¡ï¼Œè¿™å¾ˆå¥½ï¼\")\n",
    "            suggestions.append(\"ğŸ“Š å¯ä»¥ç»§ç»­æ¢ç´¢ä¸åŒä¸»é¢˜æ–‡æœ¬çš„æƒ…æ„Ÿç‰¹å¾\")\n",
    "        \n",
    "        if self.total_analyses >= 10:\n",
    "            suggestions.append(\"ğŸ† æ‚¨å·²ç»å®Œæˆäº†å¾ˆå¤šåˆ†æï¼Œæ˜¯ä¸ªæ–‡æœ¬æƒ…æ„Ÿä¸“å®¶äº†ï¼\")\n",
    "        \n",
    "        return suggestions\n",
    "\n",
    "# ============================================================================\n",
    "# 7. å¯è§†åŒ–å‡½æ•°\n",
    "# ============================================================================\n",
    "\n",
    "def create_sentiment_visualizations(df):\n",
    "    \"\"\"åˆ›å»ºæƒ…æ„Ÿåˆ†æå¯è§†åŒ–\"\"\"\n",
    "    try:\n",
    "        print(\"ğŸ“ˆ ç”Ÿæˆæƒ…æ„Ÿåˆ†å¸ƒå¯è§†åŒ–...\")\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # æƒ…æ„Ÿåˆ†ç±»é¥¼å›¾\n",
    "        sentiment_counts = df['sentiment_category'].value_counts()\n",
    "        if chinese_font_prop:\n",
    "            # ä½¿ç”¨ä¸­æ–‡å­—ä½“åˆ›å»ºé¥¼å›¾\n",
    "            wedges, texts, autotexts = axes[0,0].pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%')\n",
    "            # è®¾ç½®æ ‡ç­¾å­—ä½“\n",
    "            for text in texts:\n",
    "                text.set_fontproperties(chinese_font_prop)\n",
    "            axes[0,0].set_title('æƒ…æ„Ÿåˆ†ç±»åˆ†å¸ƒ', fontproperties=chinese_font_prop, fontsize=14)\n",
    "        else:\n",
    "            axes[0,0].pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%')\n",
    "            axes[0,0].set_title('æƒ…æ„Ÿåˆ†ç±»åˆ†å¸ƒ', fontsize=14)\n",
    "        \n",
    "        # æƒ…æ„Ÿå¾—åˆ†ç›´æ–¹å›¾\n",
    "        axes[0,1].hist(df['sentiment_score'], bins=30, alpha=0.7, color='skyblue')\n",
    "        if chinese_font_prop:\n",
    "            axes[0,1].set_title('æƒ…æ„Ÿå¾—åˆ†åˆ†å¸ƒ', fontproperties=chinese_font_prop, fontsize=14)\n",
    "            axes[0,1].set_xlabel('æƒ…æ„Ÿå¾—åˆ†', fontproperties=chinese_font_prop)\n",
    "            axes[0,1].set_ylabel('é¢‘æ¬¡', fontproperties=chinese_font_prop)\n",
    "        else:\n",
    "            axes[0,1].set_title('æƒ…æ„Ÿå¾—åˆ†åˆ†å¸ƒ', fontsize=14)\n",
    "            axes[0,1].set_xlabel('æƒ…æ„Ÿå¾—åˆ†')\n",
    "            axes[0,1].set_ylabel('é¢‘æ¬¡')\n",
    "        \n",
    "        # ä¸åŒäº‹ä»¶ç±»å‹çš„æƒ…æ„Ÿåˆ†å¸ƒ\n",
    "        event_data = []\n",
    "        event_labels = []\n",
    "        for event_type in df['event_type'].unique():\n",
    "            event_scores = df[df['event_type'] == event_type]['sentiment_score']\n",
    "            event_data.append(event_scores)\n",
    "            event_labels.append(event_type)\n",
    "        \n",
    "        axes[1,0].boxplot(event_data, labels=event_labels)\n",
    "        if chinese_font_prop:\n",
    "            axes[1,0].set_title('ä¸åŒäº‹ä»¶ç±»å‹çš„æƒ…æ„Ÿåˆ†å¸ƒ', fontproperties=chinese_font_prop, fontsize=14)\n",
    "            # è®¾ç½®xè½´æ ‡ç­¾å­—ä½“\n",
    "            for label in axes[1,0].get_xticklabels():\n",
    "                label.set_fontproperties(chinese_font_prop)\n",
    "        else:\n",
    "            axes[1,0].set_title('ä¸åŒäº‹ä»¶ç±»å‹çš„æƒ…æ„Ÿåˆ†å¸ƒ', fontsize=14)\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # å¹³å°é—´æƒ…æ„Ÿå·®å¼‚\n",
    "        platform_sentiment = df.groupby('platform')['sentiment_score'].mean().sort_values()\n",
    "        axes[1,1].bar(platform_sentiment.index, platform_sentiment.values)\n",
    "        if chinese_font_prop:\n",
    "            axes[1,1].set_title('ä¸åŒå¹³å°çš„å¹³å‡æƒ…æ„Ÿå¾—åˆ†', fontproperties=chinese_font_prop, fontsize=14)\n",
    "            # è®¾ç½®xè½´æ ‡ç­¾å­—ä½“\n",
    "            for label in axes[1,1].get_xticklabels():\n",
    "                label.set_fontproperties(chinese_font_prop)\n",
    "        else:\n",
    "            axes[1,1].set_title('ä¸åŒå¹³å°çš„å¹³å‡æƒ…æ„Ÿå¾—åˆ†', fontsize=14)\n",
    "        axes[1,1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # ä¿å­˜åˆ°outputæ–‡ä»¶å¤¹\n",
    "        os.makedirs('output', exist_ok=True)\n",
    "        plt.savefig('output/sentiment_analysis.png', dpi=300, bbox_inches='tight', \n",
    "                   facecolor='white', edgecolor='none')\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ æƒ…æ„Ÿå¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}\")\n",
    "\n",
    "def create_time_series_visualization(df):\n",
    "    \"\"\"åˆ›å»ºæ—¶é—´åºåˆ—å¯è§†åŒ–\"\"\"\n",
    "    try:\n",
    "        print(\"ğŸ“ˆ ç”Ÿæˆæ—¶é—´åºåˆ—å¯è§†åŒ–...\")\n",
    "        df['date'] = df['timestamp'].dt.date\n",
    "        daily_sentiment = df.groupby('date').agg({\n",
    "            'sentiment_score': ['mean', 'std', 'count']\n",
    "        }).reset_index()\n",
    "        daily_sentiment.columns = ['date', 'avg_sentiment', 'sentiment_std', 'post_count']\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "        \n",
    "        # æ¯æ—¥å¹³å‡æƒ…æ„Ÿå˜åŒ–\n",
    "        axes[0].plot(daily_sentiment['date'], daily_sentiment['avg_sentiment'], marker='o', alpha=0.7)\n",
    "        if chinese_font_prop:\n",
    "            axes[0].set_title('æ¯æ—¥å¹³å‡æƒ…æ„Ÿå˜åŒ–è¶‹åŠ¿', fontproperties=chinese_font_prop, fontsize=14)\n",
    "            axes[0].set_ylabel('å¹³å‡æƒ…æ„Ÿå¾—åˆ†', fontproperties=chinese_font_prop)\n",
    "        else:\n",
    "            axes[0].set_title('æ¯æ—¥å¹³å‡æƒ…æ„Ÿå˜åŒ–è¶‹åŠ¿', fontsize=14)\n",
    "            axes[0].set_ylabel('å¹³å‡æƒ…æ„Ÿå¾—åˆ†')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # æ¯æ—¥å‘å¸–é‡\n",
    "        axes[1].bar(daily_sentiment['date'], daily_sentiment['post_count'], alpha=0.7)\n",
    "        if chinese_font_prop:\n",
    "            axes[1].set_title('æ¯æ—¥å‘å¸–é‡å˜åŒ–', fontproperties=chinese_font_prop, fontsize=14)\n",
    "            axes[1].set_xlabel('æ—¥æœŸ', fontproperties=chinese_font_prop)\n",
    "            axes[1].set_ylabel('å‘å¸–æ•°é‡', fontproperties=chinese_font_prop)\n",
    "        else:\n",
    "            axes[1].set_title('æ¯æ—¥å‘å¸–é‡å˜åŒ–', fontsize=14)\n",
    "            axes[1].set_xlabel('æ—¥æœŸ')\n",
    "            axes[1].set_ylabel('å‘å¸–æ•°é‡')\n",
    "        \n",
    "        for ax in axes:\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "            # è®¾ç½®xè½´æ ‡ç­¾å­—ä½“\n",
    "            if chinese_font_prop:\n",
    "                for label in ax.get_xticklabels():\n",
    "                    label.set_fontproperties(chinese_font_prop)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # ä¿å­˜åˆ°outputæ–‡ä»¶å¤¹\n",
    "        os.makedirs('output', exist_ok=True)\n",
    "        plt.savefig('output/time_series_analysis.png', dpi=300, bbox_inches='tight',\n",
    "                   facecolor='white', edgecolor='none')\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ æ—¶é—´åºåˆ—å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}\")\n",
    "\n",
    "def create_wordcloud_and_network(df):\n",
    "    \"\"\"åˆ›å»ºè¯äº‘å’Œç½‘ç»œå›¾\"\"\"\n",
    "    try:\n",
    "        print(\"â˜ï¸ ç”Ÿæˆè¯äº‘...\")\n",
    "        \n",
    "        # è¯é¢‘åˆ†æ\n",
    "        all_text = ' '.join(df['cleaned_text'])\n",
    "        words = re.findall(r'[\\u4e00-\\u9fa5]+', all_text)\n",
    "        \n",
    "        chinese_stopwords = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'}\n",
    "        filtered_words = [word for word in words if len(word) >= 2 and word not in chinese_stopwords]\n",
    "        \n",
    "        if not filtered_words:\n",
    "            print(\"âš ï¸ æ²¡æœ‰æœ‰æ•ˆè¯æ±‡ç”Ÿæˆè¯äº‘\")\n",
    "            return\n",
    "        \n",
    "        # ç”Ÿæˆè¯äº‘\n",
    "        wordcloud_text = ' '.join(filtered_words)\n",
    "        \n",
    "        # å°è¯•æ‰¾åˆ°æ”¯æŒä¸­æ–‡çš„å­—ä½“æ–‡ä»¶\n",
    "        font_path = None\n",
    "        possible_fonts = [\n",
    "            '/System/Library/Fonts/PingFang.ttc',  # macOS\n",
    "            '/System/Library/Fonts/Hiragino Sans GB.ttc',  # macOS\n",
    "            'C:/Windows/Fonts/simhei.ttf',  # Windows\n",
    "            'C:/Windows/Fonts/msyh.ttc',  # Windows\n",
    "            '/usr/share/fonts/truetype/wqy/wqy-microhei.ttc',  # Linux\n",
    "            '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf'  # Linux fallback\n",
    "        ]\n",
    "        \n",
    "        for font_file in possible_fonts:\n",
    "            if os.path.exists(font_file):\n",
    "                font_path = font_file\n",
    "                break\n",
    "        \n",
    "        # åˆ›å»ºè¯äº‘\n",
    "        wordcloud_params = {\n",
    "            'width': 1200, \n",
    "            'height': 600, \n",
    "            'background_color': 'white',\n",
    "            'max_words': 100,\n",
    "            'collocations': False,\n",
    "            'colormap': 'viridis',\n",
    "            'relative_scaling': 0.5,\n",
    "            'min_font_size': 10\n",
    "        }\n",
    "        \n",
    "        if font_path:\n",
    "            wordcloud_params['font_path'] = font_path\n",
    "            print(f\"âœ… è¯äº‘ä½¿ç”¨å­—ä½“: {font_path}\")\n",
    "        else:\n",
    "            print(\"âš ï¸ æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“æ–‡ä»¶ï¼Œè¯äº‘ä¸­æ–‡å¯èƒ½æ˜¾ç¤ºå¼‚å¸¸\")\n",
    "        \n",
    "        wordcloud = WordCloud(**wordcloud_params).generate(wordcloud_text)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        if chinese_font_prop:\n",
    "            plt.title('ç¤¾äº¤åª’ä½“æ–‡æœ¬è¯äº‘', fontproperties=chinese_font_prop, fontsize=16)\n",
    "        else:\n",
    "            plt.title('ç¤¾äº¤åª’ä½“æ–‡æœ¬è¯äº‘', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # ä¿å­˜åˆ°outputæ–‡ä»¶å¤¹\n",
    "        os.makedirs('output', exist_ok=True)\n",
    "        plt.savefig('output/wordcloud.png', dpi=300, bbox_inches='tight',\n",
    "                   facecolor='white', edgecolor='none')\n",
    "        plt.show()\n",
    "        \n",
    "        # ç½‘ç»œå›¾\n",
    "        print(\"ğŸŒ ç”Ÿæˆå…±ç°ç½‘ç»œå›¾...\")\n",
    "        create_cooccurrence_network(df['cleaned_text'].tolist(), chinese_stopwords)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ è¯äº‘/ç½‘ç»œå›¾ç”Ÿæˆå¤±è´¥: {e}\")\n",
    "\n",
    "def create_cooccurrence_network(texts, stopwords, top_n=15):\n",
    "    \"\"\"åˆ›å»ºè¯æ±‡å…±ç°ç½‘ç»œå›¾\"\"\"\n",
    "    try:\n",
    "        all_words = []\n",
    "        for text in texts:\n",
    "            words = re.findall(r'[\\u4e00-\\u9fa5]+', text)\n",
    "            words = [w for w in words if len(w) >= 2 and w not in stopwords]\n",
    "            all_words.extend(words)\n",
    "        \n",
    "        if not all_words:\n",
    "            print(\"âš ï¸ æ²¡æœ‰æœ‰æ•ˆè¯æ±‡ç”Ÿæˆç½‘ç»œå›¾\")\n",
    "            return\n",
    "        \n",
    "        word_freq = Counter(all_words)\n",
    "        top_words = [word for word, freq in word_freq.most_common(top_n)]\n",
    "        \n",
    "        # æ„å»ºå…±ç°çŸ©é˜µ\n",
    "        cooccurrence = {word: Counter() for word in top_words}\n",
    "        \n",
    "        for text in texts:\n",
    "            words = re.findall(r'[\\u4e00-\\u9fa5]+', text)\n",
    "            words = [w for w in words if w in top_words]\n",
    "            \n",
    "            for i, word1 in enumerate(words):\n",
    "                for j in range(max(0, i-2), min(len(words), i+3)):\n",
    "                    if i != j:\n",
    "                        word2 = words[j]\n",
    "                        cooccurrence[word1][word2] += 1\n",
    "        \n",
    "        # åˆ›å»ºç½‘ç»œå›¾\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        for word in top_words:\n",
    "            G.add_node(word, size=word_freq[word])\n",
    "        \n",
    "        for word1 in top_words:\n",
    "            for word2, weight in cooccurrence[word1].most_common(3):\n",
    "                if weight > 1 and word1 != word2:\n",
    "                    G.add_edge(word1, word2, weight=weight)\n",
    "        \n",
    "        if G.number_of_nodes() == 0:\n",
    "            print(\"âš ï¸ ç½‘ç»œå›¾æ²¡æœ‰æœ‰æ•ˆèŠ‚ç‚¹\")\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "        \n",
    "        node_sizes = [G.nodes[node]['size'] * 100 for node in G.nodes()]\n",
    "        nx.draw_networkx_nodes(G, pos, node_size=node_sizes, \n",
    "                              node_color='lightblue', alpha=0.7)\n",
    "        \n",
    "        if G.number_of_edges() > 0:\n",
    "            edges = G.edges()\n",
    "            weights = [G[u][v]['weight'] for u, v in edges]\n",
    "            nx.draw_networkx_edges(G, pos, width=[w*0.5 for w in weights], \n",
    "                                  alpha=0.6, edge_color='gray')\n",
    "        \n",
    "        # ç»˜åˆ¶æ ‡ç­¾ï¼Œä½¿ç”¨ä¸­æ–‡å­—ä½“\n",
    "        if chinese_font_prop:\n",
    "            # NetworkXçš„å­—ä½“è®¾ç½®éœ€è¦ä½¿ç”¨font_familyå‚æ•°\n",
    "            font_family = chinese_font_prop.get_name()\n",
    "            nx.draw_networkx_labels(G, pos, font_size=10, font_family=font_family)\n",
    "        else:\n",
    "            nx.draw_networkx_labels(G, pos, font_size=10)\n",
    "        \n",
    "        if chinese_font_prop:\n",
    "            plt.title('è¯æ±‡å…±ç°ç½‘ç»œå›¾', fontproperties=chinese_font_prop, fontsize=16)\n",
    "        else:\n",
    "            plt.title('è¯æ±‡å…±ç°ç½‘ç»œå›¾', fontsize=16)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # ä¿å­˜åˆ°outputæ–‡ä»¶å¤¹\n",
    "        os.makedirs('output', exist_ok=True)\n",
    "        plt.savefig('output/network_analysis.png', dpi=300, bbox_inches='tight',\n",
    "                   facecolor='white', edgecolor='none')\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ç½‘ç»œå›¾ç”Ÿæˆå¤±è´¥: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 8. ä¸»ç¨‹åºæ‰§è¡Œ\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»ç¨‹åºå‡½æ•°\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"\\n1. ğŸ“Š ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®...\")\n",
    "        df = generate_social_media_data(1000)\n",
    "        print(f\"ç”Ÿæˆäº† {len(df)} æ¡ç¤¾äº¤åª’ä½“æ•°æ®\")\n",
    "        \n",
    "        print(\"\\n2. ğŸ§¹ æ–‡æœ¬æ¸…æ´—...\")\n",
    "        df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "        df = df[df['cleaned_text'].str.len() > 0]\n",
    "        print(f\"æ¸…æ´—åä¿ç•™ {len(df)} æ¡æœ‰æ•ˆæ•°æ®\")\n",
    "        \n",
    "        print(\"\\n3. ğŸ˜Š æƒ…æ„Ÿåˆ†æ...\")\n",
    "        df['sentiment_score'] = df['cleaned_text'].apply(get_sentiment_score)\n",
    "        df['sentiment_category'] = pd.cut(df['sentiment_score'], \n",
    "                                        bins=[-1, -0.1, 0.1, 1], \n",
    "                                        labels=['è´Ÿé¢', 'ä¸­æ€§', 'æ­£é¢'])\n",
    "        \n",
    "        print(\"\\n4. ğŸ“ˆ ç»Ÿè®¡åˆ†æ...\")\n",
    "        create_sentiment_visualizations(df)\n",
    "        create_time_series_visualization(df)\n",
    "        \n",
    "        print(\"\\n5. ğŸ“Š ç»Ÿè®¡è®¡ç®—åˆ†æ...\")\n",
    "        # ç»Ÿè®¡è®¡ç®—1ï¼šç›¸å…³æ€§åˆ†æ\n",
    "        df['text_length'] = df['cleaned_text'].str.len()\n",
    "        correlation = df['text_length'].corr(df['sentiment_score'])\n",
    "        \n",
    "        print(f\"æ–‡æœ¬é•¿åº¦ä¸æƒ…æ„Ÿå¾—åˆ†çš„ç›¸å…³ç³»æ•°: {correlation:.4f}\")\n",
    "        \n",
    "        # ç»Ÿè®¡è®¡ç®—2ï¼šæ–¹å·®åˆ†æ\n",
    "        user_sentiment_stats = df.groupby('user_id')['sentiment_score'].agg([\n",
    "            'mean', 'std', 'count', 'min', 'max'\n",
    "        ]).reset_index()\n",
    "        user_sentiment_stats['sentiment_range'] = user_sentiment_stats['max'] - user_sentiment_stats['min']\n",
    "        \n",
    "        active_users = user_sentiment_stats[user_sentiment_stats['count'] >= 3]\n",
    "        if len(active_users) > 0:\n",
    "            avg_volatility = active_users['std'].mean()\n",
    "            print(f\"æ´»è·ƒç”¨æˆ·å¹³å‡æƒ…æ„Ÿæ³¢åŠ¨æ€§: {avg_volatility:.4f}\")\n",
    "        else:\n",
    "            print(\"æ²¡æœ‰è¶³å¤Ÿçš„æ´»è·ƒç”¨æˆ·æ•°æ®è¿›è¡Œæ³¢åŠ¨æ€§åˆ†æ\")\n",
    "        \n",
    "        print(\"\\n6. ğŸ” ä¸»é¢˜å»ºæ¨¡...\")\n",
    "        lda_model, vectorizer, topics = perform_topic_modeling(df['cleaned_text'].tolist())\n",
    "        \n",
    "        if topics:\n",
    "            print(f\"è¯†åˆ«å‡º {len(topics)} ä¸ªä¸»è¦ä¸»é¢˜ï¼š\")\n",
    "            for i, topic in enumerate(topics):\n",
    "                print(f\"ä¸»é¢˜ {i+1}: {', '.join(topic[:5])}\")\n",
    "        \n",
    "        print(\"\\n7. ğŸ“ è¯é¢‘åˆ†æ...\")\n",
    "        all_text = ' '.join(df['cleaned_text'])\n",
    "        words = re.findall(r'[\\u4e00-\\u9fa5]+', all_text)\n",
    "        \n",
    "        chinese_stopwords = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'}\n",
    "        filtered_words = [word for word in words if len(word) >= 2 and word not in chinese_stopwords]\n",
    "        \n",
    "        word_freq = Counter(filtered_words)\n",
    "        top_words = word_freq.most_common(20)\n",
    "        \n",
    "        print(\"é«˜é¢‘è¯æ±‡ï¼ˆTop 10ï¼‰ï¼š\")\n",
    "        for i, (word, freq) in enumerate(top_words[:10]):\n",
    "            print(f\"{i+1:2d}. {word:8s} - {freq:3d} æ¬¡\")\n",
    "        \n",
    "        print(\"\\n8. â˜ï¸ è¯äº‘å’Œç½‘ç»œå›¾...\")\n",
    "        create_wordcloud_and_network(df)\n",
    "        \n",
    "        print(\"\\n9. ğŸ¤– äº¤äº’å¼æƒ…æ„Ÿåˆ†æå™¨æ¼”ç¤º...\")\n",
    "        analyzer = SentimentAnalyzer()\n",
    "        \n",
    "        demo_texts = [\n",
    "            \"ä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œå¿ƒæƒ…å¾ˆæ£’ï¼\",\n",
    "            \"è¿™ä¸ªæ¶ˆæ¯è®©æˆ‘å¾ˆå¤±æœ›\",\n",
    "            \"å·¥ä½œè¿˜ç®—æ­£å¸¸ï¼Œæ²¡ä»€ä¹ˆç‰¹åˆ«çš„\",\n",
    "            \"å¤ªæ¿€åŠ¨äº†ï¼ç»ˆäºæˆåŠŸäº†ï¼\",\n",
    "            \"æœ‰ç‚¹æ‹…å¿ƒæ˜å¤©çš„è€ƒè¯•\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\næƒ…æ„Ÿåˆ†æç»“æœï¼š\")\n",
    "        for text in demo_texts:\n",
    "            result = analyzer.analyze_sentiment(text)\n",
    "            if 'error' not in result:\n",
    "                print(f\"æ–‡æœ¬: {result['text']}\")\n",
    "                print(f\"æƒ…æ„Ÿ: {result['sentiment_label']} {result['emoji']} (å¾—åˆ†: {result['sentiment_score']:.3f})\")\n",
    "                print()\n",
    "        \n",
    "        stats = analyzer.get_statistics()\n",
    "        print(\"åˆ†æç»Ÿè®¡ï¼š\")\n",
    "        print(f\"æ€»åˆ†ææ¬¡æ•°: {stats['total_analyses']}\")\n",
    "        print(f\"æƒ…æ„Ÿåˆ†å¸ƒ: {stats['sentiment_counts']}\")\n",
    "        if 'avg_sentiment' in stats:\n",
    "            print(f\"å¹³å‡æƒ…æ„Ÿå¾—åˆ†: {stats['avg_sentiment']:.3f}\")\n",
    "        \n",
    "        suggestions = analyzer.get_personalized_suggestions()\n",
    "        print(\"\\nä¸ªæ€§åŒ–å»ºè®®ï¼š\")\n",
    "        for suggestion in suggestions:\n",
    "            print(f\"- {suggestion}\")\n",
    "        \n",
    "        print(\"\\n10. ğŸ“‹ æ•°æ®å¯é æ€§ä¸å±€é™æ€§åˆ†æ...\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"æ•°æ®å¯é æ€§åˆ†æï¼š\")\n",
    "        print(\"âœ… ä¼˜åŠ¿ï¼š\")\n",
    "        print(\"  - æ•°æ®é‡å……è¶³ï¼Œæä¾›äº†è¾ƒå¥½çš„ç»Ÿè®¡åŸºç¡€\")\n",
    "        print(\"  - æ—¶é—´è·¨åº¦è¾ƒé•¿ï¼Œèƒ½å¤Ÿè§‚å¯Ÿåˆ°è¶‹åŠ¿å˜åŒ–\")\n",
    "        print(\"  - åŒ…å«å¤šå¹³å°æ•°æ®ï¼Œå¢åŠ äº†ä»£è¡¨æ€§\")\n",
    "        \n",
    "        print(\"\\nâš ï¸ å±€é™æ€§ï¼š\")\n",
    "        print(\"  - æ¨¡æ‹Ÿæ•°æ®åŸºäºé¢„è®¾æ¨¡æ¿ï¼Œç¼ºä¹çœŸå®å¤æ‚æ€§\")\n",
    "        if USE_LITE_MODE:\n",
    "            print(\"  - è½»é‡æ¨¡å¼ä½¿ç”¨è¯å…¸åˆ†æï¼Œå‡†ç¡®æ€§æœ‰é™\")\n",
    "        else:\n",
    "            print(\"  - æƒ…æ„Ÿåˆ†æä¸»è¦é’ˆå¯¹è‹±æ–‡ä¼˜åŒ–ï¼Œä¸­æ–‡è¯†åˆ«å­˜åœ¨åå·®\")\n",
    "        print(\"  - æ— æ³•è¯†åˆ«è®½åˆºã€åè¯­ç­‰å¤æ‚è¯­è¨€ç°è±¡\")\n",
    "        print(\"  - ç›¸å…³æ€§ä¸ç­‰äºå› æœæ€§ï¼Œç¼ºä¹å› æœæ¨æ–­\")\n",
    "        \n",
    "        print(\"\\nğŸ‰ é¡¹ç›®å®Œæˆï¼\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"ä½œä¸šè¦æ±‚å®ŒæˆçŠ¶å†µï¼š\")\n",
    "        print(\"âœ… è®ºç‚¹æ„å»ºï¼ˆ20åˆ†ï¼‰ï¼šç¤¾äº¤åª’ä½“æƒ…æ„Ÿä¸ç¤¾ä¼šäº‹ä»¶å…³è”æ€§ç ”ç©¶\")\n",
    "        print(\"âœ… æäº¤æ ¼å¼è§„èŒƒï¼ˆ20åˆ†ï¼‰ï¼šå®Œæ•´çš„å¯æ‰§è¡Œä»£ç \")\n",
    "        print(\"âœ… ç»Ÿè®¡åˆ†æï¼ˆ20åˆ†ï¼‰ï¼š2ä¸ªå¯è§†åŒ– + 2é¡¹ç»Ÿè®¡è®¡ç®— + å¯é æ€§åˆ†æ\")\n",
    "        print(\"âœ… æ–‡æœ¬åˆ†æï¼ˆ20åˆ†ï¼‰ï¼šæ¸…æ´—å‡½æ•° + ä¸»é¢˜å»ºæ¨¡ + è¯é¢‘åˆ†æ + è¯äº‘ + ç½‘ç»œå›¾\")\n",
    "        print(\"âœ… äº¤äº’å¼Pythonç±»ï¼ˆ20åˆ†ï¼‰ï¼šSentimentAnalyzerç±» + å®Œæ•´æ¼”ç¤º\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"è¿è¡Œæ¨¡å¼: {'è½»é‡ç‰ˆ (åŸºäºè¯å…¸)' if USE_LITE_MODE else 'å®Œæ•´ç‰ˆ (åŒ…å«NLTK)'}\")\n",
    "        print(\"ç”Ÿæˆçš„æ–‡ä»¶ä¿å­˜åœ¨ output/ æ–‡ä»¶å¤¹ä¸­\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dec39b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
